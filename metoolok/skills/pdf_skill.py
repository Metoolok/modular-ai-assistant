import PyPDF2
import os
import re
from datetime import datetime
from collections import Counter
from .base import BaseSkill


class PDFReaderSkill(BaseSkill):
    """
    Metoolok GeliÅŸmiÅŸ PDF Analiz ModÃ¼lÃ¼

    Ã–zellikler:
        - ğŸ¤– AI Destekli AkÄ±llÄ± Ã–zet: GerÃ§ek iÃ§erik analizi
        - ğŸ“Š Otomatik Ä°Ã§indekiler Ã‡Ä±karÄ±mÄ±
        - ğŸ” GeliÅŸmiÅŸ Arama: BaÄŸlam ile birlikte sonuÃ§lar
        - ğŸ“ˆ Ä°statistiksel Analiz: Kelime frekansÄ±, sayfa analizi
        - ğŸ¯ Konu Tespit: Otomatik kategori belirleme
        - ğŸ’¾ Ã‡oklu PDF YÃ¶netimi: Birden fazla dÃ¶kÃ¼man hafÄ±zasÄ±
        - ğŸ”– Yer Ä°mi (Bookmark) DesteÄŸi
        - ğŸ“‘ Sayfa BazlÄ± Okuma
    """
    name = "pdf"
    keywords = [
        "pdf", "document", "dosya", "oku", "belge", "summary", "analiz",
        "extract", "anlat", "Ã¶zet", "iÃ§indekiler", "sayfa", "bÃ¶lÃ¼m"
    ]
    description = "PDF dÃ¶kÃ¼manlarÄ±nÄ± akÄ±llÄ±ca okur, Ã¶zetler, analiz eder ve iÃ§inde geliÅŸmiÅŸ arama yapar."

    def __init__(self, data_manager=None):
        super().__init__(data_manager)
        self.pdf_library = {}  # Ã‡oklu PDF saklama: {filename: {text, metadata, ...}}
        self.current_pdf = None  # Aktif dÃ¶kÃ¼man

    async def execute(self, args: str) -> str:
        """PDF iÅŸlemlerini yÃ¶neten ana metod."""
        args_lower = args.lower()

        # 1. HAFIZA KONTROLÃœ - Otomatik PDF yÃ¼kleme
        if not self.current_pdf and self.data_manager:
            last_file = self.data_manager.context_memory.get("last_uploaded_file")
            if last_file and last_file.endswith(".pdf"):
                self.load_pdf(last_file)

        try:
            # Durum A: PDF YÃ¼kleme
            if "/temp/" in args or args.endswith(".pdf") or "yÃ¼kle" in args_lower or "load" in args_lower:
                file_path = args.split()[-1] if " " in args else args
                return self.load_pdf(file_path)

            # Durum B: AkÄ±llÄ± Ã–zet
            if any(word in args_lower for word in ["Ã¶zet", "summary", "anlat", "analiz"]):
                return self.smart_summary()

            # Durum C: Ä°Ã§indekiler
            if "iÃ§indekiler" in args_lower or "toc" in args_lower or "baÅŸlÄ±k" in args_lower:
                return self.extract_table_of_contents()

            # Durum D: GeliÅŸmiÅŸ Arama
            if any(word in args_lower for word in ["search:", "ara:", "bul:"]):
                query = args.split(":", 1)[1].strip() if ":" in args else args.replace("pdf", "").strip()
                return self.advanced_search(query)

            # Durum E: Sayfa Okuma
            if "sayfa" in args_lower or "page" in args_lower:
                try:
                    page_num = int(''.join(filter(str.isdigit, args)))
                    return self.read_page(page_num)
                except:
                    return "âš ï¸ Sayfa numarasÄ± belirtmelisiniz. Ã–rn: `pdf sayfa 5`"

            # Durum F: Ä°statistikler
            if "istatistik" in args_lower or "stats" in args_lower:
                return self.get_statistics()

            # Durum G: Konu Analizi
            if "konu" in args_lower or "topic" in args_lower or "kategori" in args_lower:
                return self.detect_topics()

            # Durum H: DÃ¶kÃ¼man Listesi
            if "liste" in args_lower or "list" in args_lower:
                return self.list_documents()

            # Durum I: DÃ¶kÃ¼man DeÄŸiÅŸtir
            if "deÄŸiÅŸtir" in args_lower or "switch" in args_lower:
                filename = args.split()[-1]
                return self.switch_document(filename)

            return self.show_help()

        except Exception as e:
            self.logger.error(f"PDF Execute Error: {e}")
            return self.format_error(f"Ä°ÅŸlem hatasÄ±: {str(e)}")

    def load_pdf(self, file_path: str) -> str:
        """PDF'i yÃ¼kler ve geliÅŸmiÅŸ metadata Ã§Ä±karÄ±r"""
        try:
            if not os.path.exists(file_path):
                return "âŒ Dosya bulunamadÄ±."

            filename = os.path.basename(file_path)

            # PDF okuma
            with open(file_path, "rb") as f:
                reader = PyPDF2.PdfReader(f)

                # Metadata Ã§Ä±karÄ±mÄ±
                metadata = {
                    "title": reader.metadata.title if reader.metadata and reader.metadata.title else filename,
                    "author": reader.metadata.author if reader.metadata and reader.metadata.author else "Bilinmiyor",
                    "pages": len(reader.pages),
                    "created": reader.metadata.creation_date if reader.metadata and hasattr(reader.metadata,
                                                                                            'creation_date') else None
                }

                # Tam metin Ã§Ä±karÄ±mÄ±
                full_text = []
                page_texts = {}

                for i, page in enumerate(reader.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        full_text.append(page_text)
                        page_texts[i] = page_text

                combined_text = "\n".join(full_text)

                # DÃ¶kÃ¼manÄ± kÃ¼tÃ¼phaneye ekle
                self.pdf_library[filename] = {
                    "text": combined_text,
                    "pages": page_texts,
                    "metadata": metadata,
                    "loaded_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
                    "word_count": len(combined_text.split()),
                    "char_count": len(combined_text)
                }

                self.current_pdf = filename

                return (
                    f"### âœ… PDF BaÅŸarÄ±yla YÃ¼klendi\n\n"
                    f"**ğŸ“„ DÃ¶kÃ¼man:** {metadata['title']}\n"
                    f"**âœï¸ Yazar:** {metadata['author']}\n"
                    f"**ğŸ“‘ Sayfa SayÄ±sÄ±:** {metadata['pages']}\n"
                    f"**ğŸ“Š Kelime SayÄ±sÄ±:** ~{self.pdf_library[filename]['word_count']:,}\n"
                    f"**ğŸ• YÃ¼kleme:** {self.pdf_library[filename]['loaded_at']}\n\n"
                    f"ğŸ’¡ *Åimdi `pdf Ã¶zet` veya `pdf iÃ§indekiler` komutlarÄ±nÄ± kullanabilirsin!*"
                )

        except Exception as e:
            self.logger.error(f"PDF Load Error: {e}")
            return f"âŒ YÃ¼kleme hatasÄ±: {str(e)}"

    def smart_summary(self) -> str:
        """ULTRA GELÄ°ÅMÄ°Å Ã–ZET MOTORU - v3.0"""
        if not self.current_pdf:
            return "âš ï¸ Ã–nce bir PDF yÃ¼kleyin."

        doc = self.pdf_library[self.current_pdf]
        text = doc["text"]
        pages = doc["metadata"]["pages"]

        # ============ AÅAMA 1: METÄ°N TEMÄ°ZLÄ°ÄÄ° ============
        # OCR hatalarÄ±nÄ± dÃ¼zelt
        text = self._fix_ocr_errors(text)

        # ============ AÅAMA 2: YAPISAL ANALÄ°Z ============
        # BaÅŸlÄ±klarÄ± tespit et
        headers = self._detect_headers(text)

        # BÃ¶lÃ¼mlere ayÄ±r
        sections = self._split_into_sections(text, headers)

        # ============ AÅAMA 3: ANLAMLI Ä°Ã‡ERÄ°K Ã‡IKARIMI ============
        meaningful_content = []

        for section_name, section_text in sections.items():
            # Her bÃ¶lÃ¼mden en iyi paragraflarÄ± al
            paragraphs = self._extract_quality_paragraphs(section_text)

            for para in paragraphs[:2]:  # Her bÃ¶lÃ¼mden max 2 paragraf
                score = self._calculate_content_quality(para, text)
                if score > 5.0:  # Kalite eÅŸiÄŸi
                    meaningful_content.append({
                        'text': para,
                        'score': score,
                        'section': section_name
                    })

        # Skora gÃ¶re sÄ±rala
        meaningful_content.sort(key=lambda x: x['score'], reverse=True)

        if not meaningful_content:
            return self._fallback_summary(doc, text)

        # ============ AÅAMA 4: AKILLI Ã–ZET OLUÅTURMA ============
        summary_parts = {
            'intro': None,
            'main_points': [],
            'conclusion': None
        }

        # En iyi 6 iÃ§eriÄŸi seÃ§ ve kategorize et
        top_content = meaningful_content[:6]

        # Ä°lk iÃ§erik genelde giriÅŸ
        if top_content:
            summary_parts['intro'] = top_content[0]['text']

        # Orta iÃ§erikler ana noktalar
        if len(top_content) > 2:
            summary_parts['main_points'] = [item['text'] for item in top_content[1:-1]]

        # Son iÃ§erik sonuÃ§
        if len(top_content) > 1:
            summary_parts['conclusion'] = top_content[-1]['text']

        # ============ AÅAMA 5: TEMA VE KAVRAM ANALÄ°ZÄ° ============
        themes = self._extract_themes(text)
        main_concepts = self._extract_key_concepts(text)
        timeline = self._extract_timeline(text)

        # ============ AÅAMA 6: FORMATLAMA ============
        summary = f"### ğŸ“„ {doc['metadata']['title']} - DetaylÄ± Ã–zet\n\n"

        # Ana tema
        if themes:
            summary += f"**ğŸ¯ Konu:** {themes[0]}\n"

        # Zaman Ã§izelgesi varsa
        if timeline:
            summary += f"**ğŸ“… DÃ¶nem:** {', '.join(timeline[:3])}\n"

        # Ana kavramlar
        if main_concepts:
            summary += f"**ğŸ’¡ Anahtar Kavramlar:** {', '.join(main_concepts[:6])}\n"

        summary += "\n---\n\n"

        # GÄ°RÄ°Å
        if summary_parts['intro']:
            intro_clean = self._polish_text(summary_parts['intro'])
            if len(intro_clean) > 50:  # AnlamlÄ± mÄ± kontrol et
                summary += f"**ğŸ“– DÃ¶kÃ¼man HakkÄ±nda:**\n\n{intro_clean}\n\n"

        # ANA NOKTALAR
        if summary_parts['main_points']:
            summary += "**ğŸ“ Ã–nemli Bilgiler:**\n\n"
            for idx, point in enumerate(summary_parts['main_points'], 1):
                point_clean = self._polish_text(point)
                if len(point_clean) > 50:
                    # Ã‡ok uzunsa kÄ±salt
                    if len(point_clean) > 350:
                        point_clean = point_clean[:350] + "..."
                    summary += f"**{idx}.** {point_clean}\n\n"

        # SONUÃ‡
        if summary_parts['conclusion']:
            conclusion_clean = self._polish_text(summary_parts['conclusion'])
            if len(conclusion_clean) > 50:
                summary += f"**ğŸ¯ Ã–nemli SonuÃ§:**\n\n{conclusion_clean}\n\n"

        # ============ AÅAMA 7: EK BÄ°LGÄ°LER ============
        summary += "---\n\n"
        summary += "**ğŸ“Š DÃ¶kÃ¼man Bilgileri:**\n"
        summary += f"- ğŸ“„ Sayfa: {pages}\n"
        summary += f"- ğŸ“ Kelime: ~{doc['word_count']:,}\n"
        summary += f"- â±ï¸ Okuma: ~{doc['word_count'] // 200} dk\n\n"

        # KeÅŸfedilecek konular
        if headers:
            summary += "**ğŸ“‘ BaÅŸlÄ±ca Konular:**\n"
            for header in headers[:5]:
                summary += f"- {header}\n"
            summary += "\n"

        summary += (
            "ğŸ’¡ **Daha FazlasÄ± Ä°Ã§in:**\n"
            "â€¢ `pdf iÃ§indekiler` â†’ TÃ¼m baÅŸlÄ±klarÄ± gÃ¶r\n"
            "â€¢ `pdf search: [konu]` â†’ Spesifik bilgi ara\n"
            "â€¢ `pdf sayfa [no]` â†’ Belirli sayfayÄ± oku\n"
            "â€¢ `pdf konu` â†’ DetaylÄ± konu analizi"
        )

        return summary

    def _fix_ocr_errors(self, text: str) -> str:
        """OCR hatalarÄ±nÄ± dÃ¼zeltir"""
        # YaygÄ±n OCR hatalarÄ±
        fixes = {
            r'\bÄ° ttihat\b': 'Ä°ttihat',
            r'\bT erakki\b': 'Terakki',
            r'\bK kat\b': 'Kkat',
            r'\bO smanlÄ±\b': 'OsmanlÄ±',
            r'\bÄ° stanbul\b': 'Ä°stanbul',
            r'\b(\w)\s+(\w{1,2})\s+(\w)\b': r'\1\2\3',  # "M u stafa" -> "Mustafa"
        }

        for pattern, replacement in fixes.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)

        # Ã‡oklu boÅŸluklarÄ± temizle
        text = re.sub(r'\s+', ' ', text)

        return text

    def _detect_headers(self, text: str) -> list:
        """BaÅŸlÄ±klarÄ± tespit eder"""
        lines = text.split('\n')
        headers = []

        for line in lines:
            line = line.strip()

            # BoÅŸ veya Ã§ok uzun satÄ±rlarÄ± atla
            if len(line) < 5 or len(line) > 100:
                continue

            # BaÅŸlÄ±k kriterleri
            is_header = False

            # 1. Tamamen bÃ¼yÃ¼k harf
            if line.isupper() and 5 < len(line) < 60:
                is_header = True

            # 2. Rakam ile baÅŸlayan (1., 2., I., II., vb.)
            if re.match(r'^[\dIVX]+[\.\)]\s+[A-ZÃ‡ÄÄ°Ã–ÅÃœ]', line):
                is_header = True

            # 3. ÃœNÄ°TE, BÃ–LÃœM, KONU gibi kelimeler
            header_keywords = ['Ã¼nite', 'bÃ¶lÃ¼m', 'konu', 'kÄ±sÄ±m', 'fasÄ±l', 'madde']
            if any(kw in line.lower() for kw in header_keywords) and len(line.split()) < 10:
                is_header = True

            # 4. Ã‡ok az kelime + bÃ¼yÃ¼k harfle baÅŸlayan her kelime
            words = line.split()
            if len(words) <= 8 and all(w[0].isupper() for w in words if len(w) > 2):
                is_header = True

            if is_header and line not in headers:
                headers.append(line)

        return headers

    def _split_into_sections(self, text: str, headers: list) -> dict:
        """Metni bÃ¶lÃ¼mlere ayÄ±rÄ±r"""
        if not headers:
            return {"Ana Ä°Ã§erik": text}

        sections = {}
        lines = text.split('\n')
        current_section = "GiriÅŸ"
        current_content = []

        for line in lines:
            # Bu satÄ±r baÅŸlÄ±k mÄ±?
            if line.strip() in headers:
                # Ã–nceki bÃ¶lÃ¼mÃ¼ kaydet
                if current_content:
                    sections[current_section] = '\n'.join(current_content)

                # Yeni bÃ¶lÃ¼m baÅŸlat
                current_section = line.strip()
                current_content = []
            else:
                current_content.append(line)

        # Son bÃ¶lÃ¼mÃ¼ kaydet
        if current_content:
            sections[current_section] = '\n'.join(current_content)

        return sections

    def _extract_quality_paragraphs(self, text: str) -> list:
        """Kaliteli paragraflarÄ± Ã§Ä±karÄ±r"""
        # Hem \n\n hem de tek \n ile ayÄ±r
        potential_paras = []

        # Ã‡ift satÄ±r sonlarÄ±
        for p in text.split('\n\n'):
            potential_paras.append(p.strip())

        # Tek satÄ±r sonlarÄ± (kÄ±sa paragraflar iÃ§in)
        for p in text.split('\n'):
            p = p.strip()
            if len(p) > 80:  # Yeterince uzun
                potential_paras.append(p)

        # Filtreleme
        quality_paras = []
        for p in potential_paras:
            # Minimum kalite kontrolleri
            if len(p) < 80:  # Ã‡ok kÄ±sa
                continue
            if len(p) > 1000:  # Ã‡ok uzun
                continue
            if self._is_junk_paragraph(p):  # Gereksiz
                continue

            # CÃ¼mle var mÄ±?
            sentences = [s for s in re.split(r'[.!?]', p) if len(s.strip()) > 20]
            if len(sentences) < 1:
                continue

            quality_paras.append(p)

        return quality_paras

    def _calculate_content_quality(self, text: str, full_text: str) -> float:
        """Ä°Ã§erik kalitesini hesaplar"""
        score = 0.0

        # 1. Uzunluk skoru (ideal: 150-450 karakter)
        length = len(text)
        if 150 <= length <= 450:
            score += 3.0
        elif 100 <= length <= 600:
            score += 1.5

        # 2. CÃ¼mle yapÄ±sÄ±
        sentences = [s.strip() for s in re.split(r'[.!?]+', text) if len(s.strip()) > 15]
        if 2 <= len(sentences) <= 6:
            score += 2.5
        elif len(sentences) >= 1:
            score += 1.0

        # 3. Ä°Ã§erik zenginliÄŸi (farklÄ± kelime oranÄ±)
        words = text.split()
        if len(words) > 0:
            unique_ratio = len(set(words)) / len(words)
            score += unique_ratio * 3

        # 4. Ã–nemli kelimeler
        important_words = [
            'sonuÃ§', 'Ã¶nemli', 'Ã¶ncelikle', 'dolayÄ±sÄ±yla', 'bÃ¶ylece',
            'savaÅŸ', 'antlaÅŸma', 'dÃ¶nem', 'devlet', 'baÅŸlangÄ±Ã§', 'sÃ¼reÃ§'
        ]
        text_lower = text.lower()
        for word in important_words:
            if word in text_lower:
                score += 0.8

        # 5. Tarih/yÄ±l iÃ§eriyor mu?
        if re.search(r'\b1[789]\d{2}\b|\b20\d{2}\b', text):
            score += 1.5

        # 6. Ä°sim iÃ§eriyor mu?
        capital_words = re.findall(r'\b[A-ZÃ‡ÄÄ°Ã–ÅÃœ][a-zÃ¼Ã§ÄŸÄ±Ã¶ÅŸÃ¼]+\b', text)
        if len(capital_words) >= 2:
            score += 1.0

        # 7. Noktalama kullanÄ±mÄ±
        punctuation_count = text.count('.') + text.count(',') + text.count(';')
        if punctuation_count >= 3:
            score += 1.0

        # 8. Anahtar kelime yoÄŸunluÄŸu
        keywords = self.extract_keywords(full_text, 15)
        keyword_matches = sum(1 for kw in keywords if kw in text_lower)
        score += keyword_matches * 0.4

        return score

    def _extract_key_concepts(self, text: str) -> list:
        """Ã–nemli kavramlarÄ± Ã§Ä±karÄ±r"""
        # BÃ¼yÃ¼k harfli kelime gruplarÄ± (2-3 kelimelik)
        patterns = [
            r'\b[A-ZÃ‡ÄÄ°Ã–ÅÃœ][a-zÃ¼Ã§ÄŸÄ±Ã¶ÅŸÃ¼]+(?:\s+[A-ZÃ‡ÄÄ°Ã–ÅÃœ][a-zÃ¼Ã§ÄŸÄ±Ã¶ÅŸÃ¼]+){0,2}\b',
            r'\b[A-ZÃ‡ÄÄ°Ã–ÅÃœ][a-zÃ¼Ã§ÄŸÄ±Ã¶ÅŸÃ¼]+\s+[A-ZÃ‡ÄÄ°Ã–ÅÃœ][a-zÃ¼Ã§ÄŸÄ±Ã¶ÅŸÃ¼]+\b'
        ]

        concepts = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            concepts.extend(matches)

        # Frekans analizi
        concept_freq = Counter(concepts)

        # Gereksizleri filtrele
        stop_words = {'Bu', 'Bir', 'Bu Nedenle', 'Bunlar', 'BÃ¶ylece', 'Sonra', 'Ã–nce', 'Ä°lk', 'DÄ°KKAT', 'NOT'}

        filtered = [
            concept for concept, count in concept_freq.most_common(20)
            if count >= 2 and concept not in stop_words and len(concept) > 3
        ]

        return filtered[:10]

    def _extract_timeline(self, text: str) -> list:
        """Tarih/yÄ±l bilgilerini Ã§Ä±karÄ±r"""
        # YÄ±l tespiti (1800-2099)
        years = re.findall(r'\b(1[89]\d{2}|20\d{2})\b', text)
        year_freq = Counter(years)

        # En sÄ±k geÃ§en 5 yÄ±l
        return [year for year, _ in year_freq.most_common(5)]

    def _polish_text(self, text: str) -> str:
        """Metni cilalar, okunaklÄ± hale getirir"""
        # Gereksiz boÅŸluklarÄ± temizle
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()

        # CÃ¼mle baÅŸlarÄ±nÄ± dÃ¼zelt
        if text and not text[0].isupper():
            text = text[0].upper() + text[1:]

        # Noktalama dÃ¼zeltmeleri
        text = re.sub(r'\s+([.,!?;:])', r'\1', text)  # Noktalamadan Ã¶nce boÅŸluk olmasÄ±n
        text = re.sub(r'([.,!?;:])\s*', r'\1 ', text)  # Noktalamadan sonra boÅŸluk

        # Ã‡ift noktalama
        text = re.sub(r'\.{2,}', '.', text)

        # Son nokta yoksa ekle (eÄŸer tam cÃ¼mle ise)
        if text and text[-1] not in '.!?':
            if len(text.split()) > 5:  # En az 5 kelime varsa cÃ¼mle sayÄ±lÄ±r
                text += '.'

        return text

    def _fallback_summary(self, doc: dict, text: str) -> str:
        """Yedek Ã¶zet metodu - iÃ§erik Ã§Ä±karÄ±lamazsa"""
        # Ä°lk 1500 karakteri akÄ±llÄ±ca al
        preview = text[:1500]

        # Tam cÃ¼mlede bitir
        last_period = preview.rfind('.')
        if last_period > 500:
            preview = preview[:last_period + 1]

        preview = self._polish_text(preview)

        keywords = self.extract_keywords(text, 12)
        themes = self._extract_themes(text)

        return (
            f"### ğŸ“„ {doc['metadata']['title']} - Ã–zet\n\n"
            f"**ğŸ¯ Konu:** {themes[0] if themes else 'Genel'}\n"
            f"**ğŸ”‘ Anahtar Kelimeler:** {', '.join(keywords[:8])}\n\n"
            f"---\n\n"
            f"**ğŸ“– DÃ¶kÃ¼man Ã–nizleme:**\n\n"
            f"{preview}\n\n"
            f"---\n\n"
            f"**ğŸ“Š Bilgi:** {doc['metadata']['pages']} sayfa, ~{doc['word_count']:,} kelime\n\n"
            f"ğŸ’¡ *DetaylÄ± okuma: `pdf sayfa 1` veya `pdf iÃ§indekiler`*"
        )

    def _is_junk_paragraph(self, text: str) -> bool:
        """Gereksiz/anlamsÄ±z paragraf kontrolÃ¼"""
        text = text.strip()

        # Ã‡ok kÄ±sa
        if len(text) < 30:
            return True

        # Ã‡oÄŸunlukla rakam veya sembol
        alpha_chars = sum(c.isalpha() for c in text)
        if alpha_chars / max(len(text), 1) < 0.5:
            return True

        # Gereksiz tekrarlar
        words = text.split()
        if len(words) > 0 and len(set(words)) / len(words) < 0.3:
            return True

        # Sayfa numarasÄ±, baÅŸlÄ±k vb. kalÄ±plar
        junk_patterns = [
            r'^\d+\s*$',  # Sadece sayfa numarasÄ±
            r'^sayfa\s+\d+',
            r'^page\s+\d+',
            r'^www\.',
            r'^http',
            r'^\[.*\]$',  # Sadece referans
            r'^DÄ°KKAT:',  # Bilgi kutusu
            r'^NOT:',  # Not kutusu
        ]
        for pattern in junk_patterns:
            if re.match(pattern, text, re.IGNORECASE):
                return True

        return False

    def _extract_themes(self, text: str) -> list:
        """Ana temalarÄ± Ã§Ä±karÄ±r"""
        text_lower = text.lower()

        # Tema sÃ¶zlÃ¼ÄŸÃ¼
        theme_dict = {
            "Tarih - OsmanlÄ± DÃ¶nemi": ["osmanlÄ±", "padiÅŸah", "sultan", "imparatorluk", "saray"],
            "Tarih - Cumhuriyet": ["atatÃ¼rk", "mustafa kemal", "cumhuriyet", "inkÄ±lap", "meclis"],
            "Tarih - SavaÅŸlar": ["savaÅŸ", "muharebe", "cephe", "ordu", "zafer", "mÃ¼tareke", "balkan"],
            "Edebiyat": ["ÅŸair", "yazar", "roman", "ÅŸiir", "eser", "edebiyat"],
            "Bilim": ["bilim", "araÅŸtÄ±rma", "deney", "teori", "hipotez", "sonuÃ§"],
            "Felsefe": ["dÃ¼ÅŸÃ¼nce", "felsefe", "mantÄ±k", "akÄ±l", "bilgi", "hakikat"],
            "Ekonomi": ["ekonomi", "ticaret", "pazar", "para", "fiyat", "Ã¼retim"],
            "EÄŸitim": ["eÄŸitim", "Ã¶ÄŸretim", "okul", "Ã¶ÄŸrenci", "ders", "mÃ¼fredat"],
            "Hukuk": ["hukuk", "kanun", "yasa", "mahkeme", "hak", "adalet"],
            "Sanat": ["sanat", "resim", "mÃ¼zik", "tiyatro", "eser", "sanatÃ§Ä±"]
        }

        theme_scores = {}
        for theme, keywords in theme_dict.items():
            score = sum(text_lower.count(kw) for kw in keywords)
            if score > 0:
                theme_scores[theme] = score

        # SÄ±ralÄ± tema listesi
        sorted_themes = sorted(theme_scores.items(), key=lambda x: x[1], reverse=True)
        return [theme for theme, _ in sorted_themes[:3]]

    def extract_table_of_contents(self) -> str:
        """Otomatik iÃ§indekiler/baÅŸlÄ±k yapÄ±sÄ±nÄ± Ã§Ä±karÄ±r"""
        if not self.current_pdf:
            return "âš ï¸ Ã–nce bir PDF yÃ¼kleyin."

        doc = self.pdf_library[self.current_pdf]
        text = doc["text"]

        # BaÅŸlÄ±k tespit et
        headers = self._detect_headers(text)

        if not headers:
            return "ğŸ“‘ Otomatik iÃ§indekiler tespit edilemedi. DÃ¶kÃ¼man yapÄ±landÄ±rÄ±lmamÄ±ÅŸ olabilir."

        # Ä°Ã§indekiler formatÄ±
        toc_lines = ["### ğŸ“‘ Ä°Ã§indekiler\n"]
        for idx, header in enumerate(headers[:20], 1):  # Ä°lk 20 baÅŸlÄ±k
            toc_lines.append(f"{idx}. {header}")

        return "\n".join(toc_lines) + f"\n\n*Toplam {len(headers)} baÅŸlÄ±k tespit edildi.*"

    def advanced_search(self, query: str) -> str:
        """GeliÅŸmiÅŸ arama - baÄŸlam ile birlikte sonuÃ§lar"""
        if not self.current_pdf:
            return "âš ï¸ Ã–nce bir PDF yÃ¼kleyin."

        doc = self.pdf_library[self.current_pdf]
        text = doc["text"]

        # Arama algoritmasÄ±
        lines = text.split("\n")
        matches = []

        for i, line in enumerate(lines):
            if query.lower() in line.lower():
                # BaÄŸlam: Ã–nceki ve sonraki satÄ±rlar
                context_before = lines[max(0, i - 1)] if i > 0 else ""
                context_after = lines[min(len(lines) - 1, i + 1)] if i < len(lines) - 1 else ""

                matches.append({
                    "line": line.strip(),
                    "before": context_before.strip(),
                    "after": context_after.strip(),
                    "line_num": i
                })

        if not matches:
            # Fuzzy search - benzer kelimeler
            similar = self.find_similar_words(query, text)
            if similar:
                return f"ğŸ” **'{query}'** bulunamadÄ±.\n\n**Benzer:** {', '.join(similar[:5])}"
            return f"ğŸ” **'{query}'** dÃ¶kÃ¼man iÃ§inde bulunamadÄ±."

        # SonuÃ§larÄ± formatla
        result = [f"### ğŸ” '{query}' Arama SonuÃ§larÄ± ({len(matches)} eÅŸleÅŸme)\n"]

        for idx, match in enumerate(matches[:8], 1):  # Ä°lk 8 sonuÃ§
            result.append(
                f"**{idx}. SonuÃ§ (SatÄ±r {match['line_num']}):**\n"
                f"_{match['before']}_\n"
                f"**â†’ {match['line']}**\n"
                f"_{match['after']}_\n"
            )

        if len(matches) > 8:
            result.append(f"\n*...ve {len(matches) - 8} sonuÃ§ daha*")

        return "\n".join(result)

    def read_page(self, page_num: int) -> str:
        """Belirli bir sayfayÄ± okur"""
        if not self.current_pdf:
            return "âš ï¸ Ã–nce bir PDF yÃ¼kleyin."

        doc = self.pdf_library[self.current_pdf]

        if page_num < 1 or page_num > doc["metadata"]["pages"]:
            return f"âš ï¸ GeÃ§ersiz sayfa. DÃ¶kÃ¼man {doc['metadata']['pages']} sayfa iÃ§eriyor."

        page_text = doc["pages"].get(page_num, "")

        if not page_text:
            return f"âš ï¸ Sayfa {page_num} boÅŸ veya okunamÄ±yor."

        # Sayfa Ã¶zeti
        preview = page_text[:1500] if len(page_text) > 1500 else page_text

        return (
            f"### ğŸ“„ Sayfa {page_num} / {doc['metadata']['pages']}\n\n"
            f"{preview}\n\n"
            f"{'...' if len(page_text) > 1500 else ''}\n"
            f"*Kelime sayÄ±sÄ±: ~{len(page_text.split())}*"
        )

    def get_statistics(self) -> str:
        """DÃ¶kÃ¼man istatistikleri"""
        if not self.current_pdf:
            return "âš ï¸ Ã–nce bir PDF yÃ¼kleyin."

        doc = self.pdf_library[self.current_pdf]
        text = doc["text"]

        # Ä°statistikler
        words = text.split()
        unique_words = set(word.lower() for word in words if word.isalpha())
        sentences = [s for s in re.split(r'[.!?]+', text) if len(s.strip()) > 10]
        paragraphs = [p for p in text.split("\n\n") if len(p.strip()) > 20]

        # En sÄ±k kullanÄ±lan kelimeler
        word_freq = Counter(word.lower() for word in words if len(word) > 3 and word.isalpha())
        top_words = word_freq.most_common(15)

        # Sayfa baÅŸÄ±na ortalama
        avg_words_per_page = doc["word_count"] // doc["metadata"]["pages"]

        return (
            f"### ğŸ“Š {doc['metadata']['title']} - Ä°statistikler\n\n"
            f"**ğŸ“„ Genel:**\n"
            f"- Toplam Sayfa: {doc['metadata']['pages']}\n"
            f"- Toplam Kelime: {doc['word_count']:,}\n"
            f"- Benzersiz Kelime: {len(unique_words):,}\n"
            f"- Toplam CÃ¼mle: ~{len(sentences):,}\n"
            f"- Toplam Paragraf: ~{len(paragraphs):,}\n\n"
            f"**ğŸ“ˆ Ortalamalar:**\n"
            f"- Sayfa BaÅŸÄ±na Kelime: ~{avg_words_per_page}\n"
            f"- CÃ¼mle BaÅŸÄ±na Kelime: ~{doc['word_count'] // max(len(sentences), 1)}\n\n"
            f"**ğŸ” En SÄ±k KullanÄ±lan Kelimeler:**\n"
            + "\n".join([f"- {word}: {count}x" for word, count in top_words[:10]])
        )

    def detect_topics(self) -> str:
        """Otomatik konu/kategori tespiti"""
        if not self.current_pdf:
            return "âš ï¸ Ã–nce bir PDF yÃ¼kleyin."

        doc = self.pdf_library[self.current_pdf]
        text = doc["text"].lower()

        # Konu kategorileri
        topics = {
            "Matematik": ["matematik", "denklem", "formÃ¼l", "sayÄ±", "hesap", "geometri", "algebra"],
            "Fizik": ["fizik", "kuvvet", "enerji", "hareket", "hÄ±z", "momentum"],
            "Kimya": ["kimya", "molekÃ¼l", "atom", "reaksiyon", "element", "bileÅŸik"],
            "Biyoloji": ["biyoloji", "hÃ¼cre", "dna", "protein", "organizma", "evrim"],
            "Tarih": ["tarih", "savaÅŸ", "devlet", "imparatorluk", "kÃ¼ltÃ¼r", "uygarlÄ±k"],
            "Edebiyat": ["edebiyat", "roman", "ÅŸiir", "Ã¶ykÃ¼", "yazar", "eser"],
            "Teknoloji": ["teknoloji", "bilgisayar", "yazÄ±lÄ±m", "internet", "dijital", "ai", "yapay zeka"],
            "Ä°ÅŸletme": ["ÅŸirket", "pazarlama", "yÃ¶netim", "strateji", "mÃ¼ÅŸteri", "satÄ±ÅŸ"],
            "Hukuk": ["hukuk", "kanun", "mahkeme", "dava", "yargÄ±", "suÃ§"],
            "TÄ±p": ["tÄ±p", "hastalÄ±k", "tedavi", "ilaÃ§", "doktor", "saÄŸlÄ±k"]
        }

        detected = {}
        for topic, keywords in topics.items():
            score = sum(text.count(kw) for kw in keywords)
            if score > 0:
                detected[topic] = score

        if not detected:
            return "ğŸ¯ Belirgin bir konu kategorisi tespit edilemedi."

        # SÄ±ralama
        sorted_topics = sorted(detected.items(), key=lambda x: x[1], reverse=True)

        result = ["### ğŸ¯ DÃ¶kÃ¼man Konu Analizi\n"]
        for topic, score in sorted_topics[:5]:
            percentage = (score / sum(detected.values())) * 100
            result.append(f"- **{topic}**: %{percentage:.1f} (skor: {score})")

        primary_topic = sorted_topics[0][0]
        result.append(f"\nğŸ’¡ *Birincil konu: **{primary_topic}***")

        return "\n".join(result)

    def extract_keywords(self, text: str, top_n: int = 20) -> list:
        """Anahtar kelime Ã§Ä±karÄ±mÄ±"""
        # Stop words (gereksiz kelimeler)
        stop_words = {
            "bir", "bu", "ÅŸu", "ve", "veya", "ile", "iÃ§in", "gibi", "da", "de",
            "ki", "mi", "mu", "mÄ±", "mÃ¼", "daha", "Ã§ok", "az", "var", "yok",
            "the", "a", "an", "and", "or", "but", "in", "on", "at", "to", "for",
            "olan", "olarak", "sonra", "iÃ§in", "kadar", "gÃ¶re", "karÅŸÄ±"
        }

        words = re.findall(r'\b[a-zÃ¼Ã§ÄŸÄ±Ã¶ÅŸA-ZÃœÃ‡ÄÄ°Ã–Å]{4,}\b', text.lower())
        filtered = [w for w in words if w not in stop_words]

        word_freq = Counter(filtered)
        return [word for word, _ in word_freq.most_common(top_n)]

    def find_similar_words(self, query: str, text: str) -> list:
        """Benzer kelime Ã¶nerileri (basit fuzzy search)"""
        words = set(re.findall(r'\b[a-zÃ¼Ã§ÄŸÄ±Ã¶ÅŸA-ZÃœÃ‡ÄÄ°Ã–Å]{3,}\b', text.lower()))
        query = query.lower()

        similar = []
        for word in words:
            # Basit benzerlik: ortak harfler
            if len(set(query) & set(word)) >= min(len(query), len(word)) * 0.6:
                similar.append(word)

        return sorted(similar)[:10]

    def list_documents(self) -> str:
        """YÃ¼klÃ¼ dÃ¶kÃ¼manlarÄ± listeler"""
        if not self.pdf_library:
            return "ğŸ“š HenÃ¼z yÃ¼klÃ¼ dÃ¶kÃ¼man yok."

        result = ["### ğŸ“š YÃ¼klÃ¼ DÃ¶kÃ¼manlar\n"]
        for filename, doc in self.pdf_library.items():
            active = "âœ…" if filename == self.current_pdf else "  "
            result.append(
                f"{active} **{doc['metadata']['title']}**\n"
                f"   â†³ {doc['metadata']['pages']} sayfa, {doc['word_count']:,} kelime\n"
                f"   â†³ {doc['loaded_at']}\n"
            )

        return "\n".join(result) + "\n\nğŸ’¡ *DÃ¶kÃ¼man deÄŸiÅŸtir: `pdf deÄŸiÅŸtir dosya_adÄ±.pdf`*"

    def switch_document(self, filename: str) -> str:
        """Aktif dÃ¶kÃ¼manÄ± deÄŸiÅŸtirir"""
        if filename not in self.pdf_library:
            available = ", ".join(self.pdf_library.keys())
            return f"âš ï¸ '{filename}' bulunamadÄ±.\n\n**Mevcut:** {available}"

        self.current_pdf = filename
        doc = self.pdf_library[filename]
        return f"âœ… Aktif dÃ¶kÃ¼man: **{doc['metadata']['title']}**"

    def show_help(self) -> str:
        """YardÄ±m menÃ¼sÃ¼"""
        return (
            "### ğŸ“š Metoolok PDF - KullanÄ±m KÄ±lavuzu\n\n"
            "**ğŸ“¥ YÃ¼kleme:**\n"
            "`pdf yÃ¼kle dosya.pdf` - PDF yÃ¼kle\n\n"
            "**ğŸ“– Okuma:**\n"
            "`pdf Ã¶zet` - AkÄ±llÄ± Ã¶zet Ã§Ä±kar\n"
            "`pdf iÃ§indekiler` - BaÅŸlÄ±klarÄ± listele\n"
            "`pdf sayfa 5` - Belirli sayfayÄ± oku\n\n"
            "**ğŸ” Arama:**\n"
            "`pdf search: konu` - GeliÅŸmiÅŸ arama\n"
            "`pdf ara: kelime` - BaÄŸlamÄ±yla ara\n\n"
            "**ğŸ“Š Analiz:**\n"
            "`pdf istatistik` - DetaylÄ± istatistikler\n"
            "`pdf konu` - Otomatik konu tespiti\n\n"
            "**ğŸ“š YÃ¶netim:**\n"
            "`pdf liste` - YÃ¼klÃ¼ dÃ¶kÃ¼manlar\n"
            "`pdf deÄŸiÅŸtir dosya.pdf` - DÃ¶kÃ¼man deÄŸiÅŸtir\n\n"
            "ğŸ’¡ *TÃ¼m komutlar iÃ§in 'pdf' Ã¶n eki kullan!*"
        )